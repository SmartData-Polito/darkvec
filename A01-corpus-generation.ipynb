{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-issue",
   "metadata": {},
   "source": [
    "# <b><center>DarkVec: Automatic Analysis of Darknet Trafficwith Word Embeddings</center></b>\n",
    "## <b><center>Appendix1: Corpus Generation</center></b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-cover",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# <b>Table of Content</b> <a name=\"toc\"></a>\n",
    "* [<b>State of Art Comparison</b>](#corpus)  \n",
    "    * [DarkVec 5 Days](#c_darkvec)  \n",
    "    * [DarkVec 30 Days Per-Service Language](#c_dante)  \n",
    "    * [Dante 5 Days](#c_ip2vec)  \n",
    "    * [IP2VEC 5 Days](#c_ip2vec)\n",
    "* [<b>Grid Search for $ùëò$</b>](#model)  \n",
    "    * [Darkvec 30 days Single Language](#m_darkvec)  \n",
    "    * [Darkvec 30 days Auto Language](#m_dante)  \n",
    "    * [Darkvec 30 days Per-Service Language](#m_ip2vec)  \n",
    "    \n",
    "    \n",
    "In this notebook we provide the snippets used to generate the corpus fed to the Word2Vec model. We generate corpus for two different experiments:\n",
    "\n",
    "1) State of art comparison, in which we compare our methodology with DANTE[ref] and IP2VEC[ref]\n",
    "\n",
    "2) Grid Search for $k$, in which we generate the models for different languages\n",
    "\n",
    "<b>Note.</b> This notebooks is designed for running on a Spark cluster. Furthermore, according to the data size, the computational times are quite expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "following-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-finding",
   "metadata": {},
   "source": [
    "If the `SAVE` flag is set to `True`, then the processed corpus is saved ***ADD CORPUS PATH***. Thus, with the following snippet, the corpora we provide are saved moved to a backup folder `corpus.BAK` and all the `.txt.gz` or `.npz` files are removed. At the bottom of the notebook we report a [snippet](#restore) for restoring our corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    os.system(f'cp -r {CORPUS} {CORPUS}.BAK')\n",
    "    folders = ['dante5', 'darkvec30single', 'darkvec5xserv',\n",
    "               'darkvec30auto', 'darkvec30xserv', 'ip2vec5']\n",
    "    for ff in folders:\n",
    "        os.system(f'rm {CORPUS}/{ff}/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "packed-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_windows(hours):\n",
    "    import numpy as np\n",
    "    cnt = 0\n",
    "    slice_ = []\n",
    "    slices = []\n",
    "    hours = np.asarray(hours)\n",
    "    for h in hours:\n",
    "        slice_.append(h)\n",
    "        cnt+=1\n",
    "        if cnt == 4:\n",
    "            cnt = 0\n",
    "            slices.append(slice_)\n",
    "            slice_ = []\n",
    "    if hours.shape[0]%4!=0:\n",
    "        slices.append(list(hours[-(hours.shape[0]%4):]))\n",
    "    date_map = {}\n",
    "    for i in range(len(slices)):\n",
    "        for date in slices[i]:\n",
    "            date_map[date] = f'w_{i}'\n",
    "    \n",
    "    return date_map\n",
    "\n",
    "def load_data(partition): \n",
    "    \"\"\"Distributed function to load the traces skipping the file header\n",
    "    \"\"\"\n",
    "    for l in partition:\n",
    "        if l.startswith(\"ts\"): \n",
    "            continue    \n",
    "        yield l.split(\" \")\n",
    "\n",
    "def convert_fields(partition):\n",
    "    \"\"\"Distributed function to convert the protocols of the traces from \n",
    "    the decimal notation to human readable version\n",
    "    \"\"\"\n",
    "    for (ts, s_ip, d_ip, port, proto) in partition:\n",
    "        try:\n",
    "            proto = PROTO_CONVERSION[proto]\n",
    "        except:\n",
    "            proto = 'oth'\n",
    "\n",
    "        yield (ts, s_ip, d_ip, port, proto)\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# Corpus Generation\n",
    "#==============================================================================\n",
    "def convert_timestamp(partition):\n",
    "    \"\"\"Convert a timestamp to the datetime format, then\n",
    "    extract the hours of the day and port/protocol pairs\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    for (ts, s_ip, d_ip, port, proto) in partition:\n",
    "        new_ts = datetime.fromtimestamp(float(ts))\n",
    "        hhdd = f'{new_ts.year}_{new_ts.month}_{new_ts.day}_{new_ts.hour}'\n",
    "        dd = f'{new_ts.year}_{new_ts.month}_{new_ts.day}'\n",
    "        pair = f'{port}_{proto}'\n",
    "        \n",
    "        yield ((pair, hhdd, dd), (ts, s_ip))\n",
    "        \n",
    "def extract_services(partition):\n",
    "    import json\n",
    "    SERVICES = ['ftp', 'telnet', 'p2p', 'netbios-smb', 'mail', 'dbs', \n",
    "                'http', 'ssh', 'netbios', 'proxy', 'dns', 'kerberos', \n",
    "                'unk_sys', 'unk_usr', 'unk_eph']\n",
    "    # Services division by ports\n",
    "    with open(f'{DATA}/services/services.json', 'r') as file:\n",
    "        CLASSES = json.loads(file.read())\n",
    "        \n",
    "    def unknown_class(x):\n",
    "        \"\"\"Manage the unclassified ports. Three unknown ports ranges are\n",
    "        used: \n",
    "        System:  0 <= port <= 1023\n",
    "        User:  1024 <= port <= 49151\n",
    "        Ephemeral:  49152 <= port <= 65535\n",
    "        \"\"\"\n",
    "        x = x.split('/')[0]\n",
    "        #System Ports\n",
    "        if x!='-':\n",
    "            if int(x) >= 0 and int(x) <= 1023: return 'sys'\n",
    "            # User Ports\n",
    "            elif int(x) >= 1024 and int(x) <= 49151:return 'usr'\n",
    "            # Ephemeral Ports\n",
    "            elif int(x) >= 49152 and int(x) <= 65535:return 'eph'\n",
    "        else:\n",
    "            return 'icmp'\n",
    "    \n",
    "    for ((x, hh, dd),(ts, ip)) in partition:\n",
    "        if x in CLASSES: \n",
    "            service = CLASSES[x]\n",
    "        else: \n",
    "            service = f'unk_{unknown_class(x)}'\n",
    "        \n",
    "        yield ((service, hh, dd),(ts, ip))\n",
    "        \n",
    "def check_dayflows(flows_to_filter):\n",
    "    def run(partition):\n",
    "        import numpy as np\n",
    "        for ip, v_list in partition:\n",
    "            x = np.where(np.asarray(v_list)>flows_to_filter)[0].shape[0]\n",
    "            if flows_to_filter!=0:\n",
    "                if x>=3:\n",
    "                    yield ip\n",
    "            else:\n",
    "                yield ip\n",
    "    return run\n",
    "\n",
    "def get_slices(partition):\n",
    "    \"\"\"The keys are the port/prtocol pairs and the hours of\n",
    "    the day. Sort the IPs according to the timestamp\n",
    "    \"\"\"\n",
    "    for k1, (k, v) in partition:\n",
    "        v.sort(key=lambda x:x[0])\n",
    "        v = [x[1] for x in v]\n",
    "        yield (k1, (k, v))\n",
    "        \n",
    "        \n",
    "def get_sentences(method, save=False):\n",
    "    def run(partition):   \n",
    "        \"\"\"Split each file in sentences. One sentence per hour.\n",
    "        Then save the file in corpus file\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import gzip\n",
    "\n",
    "        def save_corpus(corpus_method, fname, sentences):\n",
    "            file = gzip.open(f'{CORPUS}/{corpus_method}/{fname}.txt.gz', 'wt')\n",
    "            for line in sentences:\n",
    "                txt = \"\"\n",
    "                for token in line:\n",
    "                    txt+=token\n",
    "                    txt+=\" \"\n",
    "                txt = txt[:-1]+\"\\n\"\n",
    "                file.write(txt)\n",
    "\n",
    "        def zero_padding_fname(day_limit):\n",
    "            yyyy, mm, dd, hh = day_limit.split('_')\n",
    "\n",
    "            if len(mm)==1: mm='0'+mm\n",
    "            if len(dd)==1: dd='0'+dd\n",
    "            if len(hh)==1: hh='0'+hh\n",
    "\n",
    "            day_limit = f'{yyyy}_{mm}_{dd}_{hh}'\n",
    "\n",
    "            return day_limit\n",
    "\n",
    "        for k, v in partition:\n",
    "            v = [(zero_padding_fname(hour), ip_list) for hour, ip_list in v]\n",
    "            v.sort(key=lambda x:x[0])\n",
    "            day_limit = v[-1][0]\n",
    "            \n",
    "            v = [np.asarray(x[1]) for x in v]\n",
    "                \n",
    "            if method in ['darkvec30xserv', 'darkvec30auto', 'darkvec5xserv']:\n",
    "                fname = f'{day_limit}_{k[0]}'\n",
    "            else:\n",
    "                fname = f'{day_limit}'\n",
    "            if save:\n",
    "                save_corpus(corpus_method = method, \n",
    "                            fname = fname, \n",
    "                            sentences = v)\n",
    "\n",
    "            yield fname\n",
    "    \n",
    "    return run\n",
    "\n",
    "def sort_by_ts(partition):\n",
    "    for k, v in partition:\n",
    "        v.sort(key=lambda x:x[0])\n",
    "        v = [x[1] for x in v]\n",
    "        if k[-2] == '_':\n",
    "            k1 = k[:-2]\n",
    "        else:\n",
    "            k1 = k[:-3]\n",
    "        yield(k1,(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-trader",
   "metadata": {},
   "source": [
    "## State of Art Comparison\n",
    "\n",
    "We compare our methodology with DANTE[ref] and IP2VEC[ref]. For each methodology we analyze 5 and 30 days of traffic. Since only Darkvec was able to finish the training, we report the corpus generation for:\n",
    "\n",
    "- Darkvec 5 days\n",
    "- Darkvec 30 days\n",
    "- Dante 5 days\n",
    "- IP2VEC 5 days\n",
    "    \n",
    "By changing the experiment parameters it is possible to extend the generation to other experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-roman",
   "metadata": {},
   "source": [
    "### DarkVec 5 days\n",
    "\n",
    "According to the time requiring processing, we comment the correct line \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "which processes only the last two days of traffic taking less time. This is only for demonstrative purposes.\n",
    "\n",
    "For running the original experiment, please comment\n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "drawn-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "\n",
    "bro = raw.mapPartitions(load_data).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "direct-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = bro.map(lambda x: (x[2], 1)).reduceByKey(lambda x,y: x+y)\\\n",
    "          .filter(lambda x:x[1]>=10).map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "contained-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "_1 = bro.map(lambda x: (x[0], x[2], x[4], x[5], x[6])).mapPartitions(convert_fields)\\\n",
    "        .mapPartitions(convert_timestamp).filter(lambda x: x[1][1] in filt).map(lambda x: ((x[0][0]\\\n",
    "        .replace('_', '/'), x[0][1], x[0][2]), x[1])).mapPartitions(extract_services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "_1.groupByKey().map(lambda x: (x[0], list(x[1]))).map(lambda x: ((x[0][0], x[0][2]), (x[0][1], x[1])))\\\n",
    "  .mapPartitions(get_slices).groupByKey().map(lambda x: (x[0], list(x[1])))\\\n",
    "  .mapPartitions(get_sentences(method='darkvec5xserv', save=SAVE)).collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-criticism",
   "metadata": {},
   "source": [
    "### Dante 5 days\n",
    "According to the time requiring processing, we comment the correct line \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "which processes only the last two days of traffic taking less time. This is only for demonstrative purposes.\n",
    "\n",
    "For running the original experiment, please comment\n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "transsexual-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp_DE(partition):\n",
    "    \"\"\"Convert a timestamp to the datetime format, then\n",
    "    extract the hours of the day and port/protocol pairs\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    for (ts, s_ip, port) in partition:\n",
    "        new_ts = datetime.fromtimestamp(float(ts))\n",
    "        hhdd = f'{new_ts.year}_{new_ts.month}_{new_ts.day}_{new_ts.hour}'\n",
    "        dd = f'{new_ts.year}_{new_ts.month}_{new_ts.day}'\n",
    "        \n",
    "        yield (hhdd, (s_ip, (ts, port)))\n",
    "        \n",
    "def zero_padding_DE(partition):\n",
    "    for (hhdd, (s_ip, (ts, port))) in partition:\n",
    "        yyyy, mm, dd, hh = hhdd.split('_')\n",
    "\n",
    "        if len(mm)==1: mm='0'+mm\n",
    "        if len(dd)==1: dd='0'+dd\n",
    "        if len(hh)==1: hh='0'+hh\n",
    "\n",
    "        rebuilt = f'{yyyy}_{mm}_{dd}_{hh}'\n",
    "\n",
    "        yield (rebuilt, (s_ip,(ts, port)))\n",
    "\n",
    "def date_to_window(date_map):\n",
    "    def run(partition):\n",
    "        for k, v in partition:\n",
    "            yield(date_map[k], v)\n",
    "    return run\n",
    "\n",
    "def sort_ports(partition):\n",
    "    for k, v in partition:\n",
    "        v.sort(key=lambda x: x[0])\n",
    "        v = [x[1] for x in v]\n",
    "        \n",
    "        yield (k, v)\n",
    "    \n",
    "def process_window(method, save = False):\n",
    "    import gzip\n",
    "    def run(partition):\n",
    "        for k, v in partition:\n",
    "            a, b = k.split('_')\n",
    "            if len(b) == 1:\n",
    "                k = f'{a}_00{b}'\n",
    "            elif len(b) == 2:\n",
    "                k = f'{a}_0{b}'\n",
    "                \n",
    "            if save:\n",
    "                file = gzip.open(f'{CORPUS}/dante5/{k}.txt.gz', 'wt')\n",
    "            string = ''\n",
    "            cnt = 0\n",
    "            for (s_ip, p_list) in v:\n",
    "                if len(p_list) > 2:\n",
    "                    cnt+=1\n",
    "                    for port in p_list:\n",
    "                        string += port\n",
    "                        string += ' '\n",
    "                    string = string[:-1]\n",
    "                    string+='\\n'\n",
    "                    if save:\n",
    "                        file.write(string)\n",
    "\n",
    "            yield k, cnt\n",
    "    \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "electrical-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "bro = raw.mapPartitions(load_data).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "synthetic-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = bro.map(lambda x: (x[2], 1)).reduceByKey(lambda x,y: x+y)\\\n",
    "          .filter(lambda x:x[1]>10).map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "_2 = bro.map(lambda x: (x[0], x[2], x[4], x[5], x[6])).mapPartitions(convert_fields)\\\n",
    "        .map(lambda x: (x[0], x[1], x[2], x[3])).filter(lambda x: x[1] in filt)\\\n",
    "        .map(lambda x: (x[0], x[1], x[3])).mapPartitions(convert_timestamp_DE)\\\n",
    "        .mapPartitions(zero_padding_DE)\n",
    "hours = _2.groupByKey().map(lambda x: x[0]).collect()\n",
    "date_map = get_time_windows(hours)\n",
    "_2.mapPartitions(date_to_window(date_map)).map(lambda x: ((x[0], x[1][0]), x[1][1]))\\\n",
    "  .groupByKey().map(lambda x: (x[0], list(x[1]))).sortByKey()\\\n",
    "  .mapPartitions(sort_ports).map(lambda x: (x[0][0], (x[0][1], x[1])))\\\n",
    "  .groupByKey().map(lambda x: (x[0], list(x[1])))\\\n",
    "  .mapPartitions(process_window(method = 'dante5', save=SAVE))\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-brown",
   "metadata": {},
   "source": [
    "### IP2VEC 5 days\n",
    "According to the time requiring processing, we comment the correct line \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "which processes only the last two days of traffic taking less time. This is only for demonstrative purposes.\n",
    "\n",
    "For running the original experiment, please comment\n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intended-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(partition):\n",
    "    for sip, dip, dp, p in partition:\n",
    "        yield (sip, dip)\n",
    "        yield (sip, dp)\n",
    "        yield (sip, p)\n",
    "        yield (dp, dip)\n",
    "        yield (p, dip)\n",
    "\n",
    "def save_data(partition):\n",
    "    import numpy as np\n",
    "    import random\n",
    "    \n",
    "    chunk_id = random.randint(1, 100)\n",
    "    X = []\n",
    "    Y = []\n",
    "    try:\n",
    "        for x, y in partition:\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        np.savez_compressed(f'{CORPUS}/ip2vec5/{X[0]}{Y[0]}{Y[1]}{Y[2]}{Y[3]}', x=X, y=Y)\n",
    "        \n",
    "        yield len(X)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "loved-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "bro = raw.mapPartitions(load_data).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerous-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = bro.map(lambda x: (x[2], 1)).reduceByKey(lambda x,y: x+y)\\\n",
    "          .filter(lambda x:x[1]>=10).map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "equal-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "_0 = bro.map(lambda x: (x[0], x[2], x[4], x[5], x[6])).filter(lambda x: x[1] in filt)\\\n",
    "        .mapPartitions(convert_fields)\\\n",
    "        .map(lambda x: (x[1], x[2], x[3], x[4])).map(lambda x: (x, 1))\\\n",
    "        .reduceByKey(lambda x,y:x+y).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "changed-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = _0.mapPartitions(gen_samples)\n",
    "step1 = step1.coalesce(50)\n",
    "step1.mapPartitions(save_data).collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "expressed-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cnt = 1\n",
    "str_cnt = '01'\n",
    "for _file in os.listdir(CORPUS+'/ip2vec5'):\n",
    "    if doc_cnt<=9: str_cnt = f'0{doc_cnt}'\n",
    "    else: str_cnt = doc_cnt\n",
    "    os.system(f\"mv {CORPUS}/ip2vec5/{_file} {CORPUS}/ip2vec5/doc{str_cnt}.npz\")\n",
    "    doc_cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-confidentiality",
   "metadata": {},
   "source": [
    "## Grid Search for $k$\n",
    "\n",
    "Grid Search for $k$. According to the language definition provided in the paper, we generate the DarkVec corpus for 30 days. Then for each language we generate a different corpus. Thus we generate:\n",
    "\n",
    "- Darkvec 30 days single language. The corpus is the sequence of the IPs as they reach the darknet\n",
    "- Darkvec 30 days auto-defined language. 11 languages (top-10 ports/protocol pairs plus one for the others). For each language the corpus is the sequence of the IPs as they reach the respecive darknet port/protocol pair\n",
    "- Darkvec 30 days per-service language. Knowledge-based languages (`services/services.json`). For each language the corpus is the sequence of the IPs as they reach the respecive darknet port/protocol pair set\n",
    "\n",
    "\n",
    "According to the time requiring processing, we comment the correct line \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "which processes only the last two days of traffic taking less time. This is only for demonstrative purposes.\n",
    "\n",
    "For running the original experiment, please comment\n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "`\n",
    "\n",
    "replacing it with \n",
    "\n",
    "`\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-5:]))\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-gothic",
   "metadata": {},
   "source": [
    "Distribute raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "studied-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw = sc.textFile(DEBUG)\n",
    "raw = sc.textFile(','.join(DEBUG.split(',')[-2:]))\n",
    "bro = raw.mapPartitions(load_data).repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proprietary-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = bro.map(lambda x: (x[0], x[2], x[4], x[5], x[6]))\\\n",
    "           .mapPartitions(convert_fields)\\\n",
    "           .mapPartitions(convert_timestamp)\n",
    "step1_1 = step1.map(lambda x: ((x[0][0].replace('_', '/'), x[0][1], x[0][2]), x[1]))\\\n",
    "       .mapPartitions(extract_services)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-retention",
   "metadata": {},
   "source": [
    "### Darkvec 30 days Single Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1.map(lambda x: (x[0][1],(x[1][0], x[1][1]))).groupByKey()\\\n",
    "     .map(lambda x:( x[0], list(x[1]))).mapPartitions(sort_by_ts)\\\n",
    "     .groupByKey().map(lambda x: (x[0], list(x[1])))\\\n",
    "     .mapPartitions(get_sentences(method='darkvec30single', save=SAVE)).collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-sunrise",
   "metadata": {},
   "source": [
    "### Darkvec 30 days Auto Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respective-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ports(top10):\n",
    "    def run(partition):\n",
    "        for ((pp, hh, dd),(ts, ip)) in partition:\n",
    "            if pp not in top10:\n",
    "                pp = 'oth'\n",
    "        \n",
    "            yield ((pp, hh, dd),(ts, ip))\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "objective-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = step1.map(lambda x: (x[0][0], 1)).reduceByKey(lambda x, y: x+y)\\\n",
    "          .sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "top10 = set([x[0] for x in top10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1.mapPartitions(select_ports(top10)).groupByKey().map(lambda x: (x[0], list(x[1])))\\\n",
    "     .map(lambda x: ((x[0][0], x[0][2]), (x[0][1], x[1]))).mapPartitions(get_slices)\\\n",
    "     .groupByKey().map(lambda x: (x[0], list(x[1])))\\\n",
    "     .mapPartitions(get_sentences(method='darkvec30auto', save=SAVE)).collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-transparency",
   "metadata": {},
   "source": [
    "### Darkvec 30 days Per-Service Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mighty-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_to_filter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "steady-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = step1_1.map(lambda x: ((x[0][2], x[1][1]), 1)).reduceByKey(lambda x,y: x+y)\\\n",
    "              .map(lambda x: (x[0][1], x[1])).groupByKey()\\\n",
    "              .map(lambda x: (x[0], list(x[1])))\\\n",
    "              .mapPartitions(check_dayflows(flows_to_filter)).collect()\n",
    "filt = set(filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-ethernet",
   "metadata": {},
   "source": [
    "Keep processing. \n",
    "Apply filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_1.filter(lambda x: x[1][1] in filt).groupByKey()\\\n",
    "        .map(lambda x: (x[0], list(x[1])))\\\n",
    "        .map(lambda x: ((x[0][0], x[0][2]), (x[0][1], x[1])))\\\n",
    "        .mapPartitions(get_slices)\\\n",
    "        .groupByKey().map(lambda x: (x[0], list(x[1])))\\\n",
    "        .mapPartitions(get_sentences(method='darkvec30xserv', save=SAVE))\\\n",
    "        .collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-scroll",
   "metadata": {},
   "source": [
    "### Restore Originals <a id='restore'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'rm -rf {CORPUS}')\n",
    "os.system(f'mv {CORPUS}.BAK {CORPUS}');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Yarn)",
   "language": "python",
   "name": "pyspark_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
