{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opened-linux",
   "metadata": {},
   "source": [
    "# <b><center>DarkVec: Automatic Analysis of Darknet Trafficwith Word Embeddings</center></b>\n",
    "## <b><center>Appendix 2: Models Training</center></b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-hunger",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# <b>Table of Content</b> <a id=\"toc_\"></a>\n",
    "\n",
    "* [<b>DarkVec Training</b>](#darkvec)  \n",
    "    * [Per-Service Language](#xserv)  \n",
    "    * [Auto-Defined Language](#auto)  \n",
    "    * [Single Language](#single)  \n",
    "* [<b>DarkVec 5 Days</b>](#darkvec5)\n",
    "* [<b>DANTE training</b>](#dante)  \n",
    "* [<b>IP2VEC training</b>](#ip2vec)  \n",
    "\n",
    "In this notebook we report the snippets for training and saving the models used in the paper. The corpus is loaded from the `CORPUS` path in the configuration file. Models and scalers are saved in the `MODELS` path in the configuration file.\n",
    "\n",
    "The model names are related to the parameters _C_, or context window size, and _V_ or embeddings size. Namely, they are:\n",
    "* `single_cC_vV_iter20`: DarkVec model trained on 30 days. Single language;\n",
    "* `auto_cC_vV_iter20`: DarkVec model trained on 30 days. Auto-defined languages;\n",
    "* `service_cC_vV_iter20`: DarkVec model trained on 30 days. Per-service languages;\n",
    "* `fivedays_c25_v50_iter20`: DarkVec model trained on 5 days. Per-service languages;\n",
    "* `ip2vec5embedder`: keras embedder generated through our implementation following the IP2VEC paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recent-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import multiprocessing\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Reshape, Dot\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-spell",
   "metadata": {},
   "source": [
    "# <b>Darkvec Training</b> <a name=\"darkvec\"></a>  \n",
    "\n",
    "[Back to Index](#toc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expired-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrative = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-miami",
   "metadata": {},
   "source": [
    "### <b>Per-Service Language</b>  <a name=\"xserv\"></a>\n",
    "\n",
    "[Back to Index](#toc_)\n",
    "\n",
    "Grid search DarkVec training over 30 days of traffic. Per-service language means that each sequence of IPs is extracted with respect to the classes of service (port/protocol) pairs. The language definition is stored in the `SERVICES` path of the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    Cs = [5, 25, 50, 75]\n",
    "    Vs = [50, 100, 150, 200]\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 30\n",
    "LANGUAGE = 'xserv'\n",
    "\n",
    "for C in Cs:\n",
    "    for V in Vs:\n",
    "        model_name = f'service_c{C}_v{V}_iter{EPOCHS}'\n",
    "        model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "        # Define the path for streaming training\n",
    "        x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "        # Init a new model\n",
    "        model = Word2Vec(min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                         sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "        # Build the vocabulary\n",
    "        model.build_vocab(sentences=x)\n",
    "        vocab_size = len(model.wv.vocab)\n",
    "        tot_examples = model.corpus_count\n",
    "        # Train the model\n",
    "        model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "                    queue_factor=2, report_delay=0.0)\n",
    "        if not demonstrative:\n",
    "            model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-experiment",
   "metadata": {},
   "source": [
    "### <b>Auto-Defined Language</b> <a id=\"auto\"></a>\n",
    "[Back to Index](#toc_)\n",
    "\n",
    "Grid search DarkVec training over 30 days of traffic. Aut-defined language means that each sequence of IPs is extracted with respect to top-10 (port/protocol) pairs. All the other pairs are classified as _oth_. The language definition is stored in the `SERVICES` path of the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    Cs = [5, 25, 50, 75]\n",
    "    Vs = [50, 100, 150, 200]\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 30\n",
    "LANGUAGE = 'auto'\n",
    "\n",
    "for C in Cs:\n",
    "    for V in Vs:\n",
    "        model_name = f'auto_c{C}_v{V}_iter{EPOCHS}'\n",
    "        model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "        # Define the path for streaming training\n",
    "        x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "        # Init a new model\n",
    "        model = Word2Vec(min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                         sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "        # Build the vocabulary\n",
    "        model.build_vocab(sentences=x)\n",
    "        vocab_size = len(model.wv.vocab)\n",
    "        tot_examples = model.corpus_count\n",
    "        # Train the model\n",
    "        model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "                    queue_factor=2, report_delay=0.0)\n",
    "        if not demonstrative:\n",
    "            model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-blair",
   "metadata": {},
   "source": [
    "### <b>Single Language</b> <a name=\"single\"></a>\n",
    "[Back to Index](#toc_)\n",
    "\n",
    "Grid search DarkVec training over 30 days of traffic. Single language means that each sequence of IPs is extracted exactly as they reached the darknet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    C = 75\n",
    "    V = 50\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 30\n",
    "LANGUAGE = 'single'\n",
    "\n",
    "model_name = f'single_c{C}_v{V}_iter{EPOCHS}'\n",
    "model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "# Define the path for streaming training\n",
    "x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "# Init a new model\n",
    "model = Word2Vec(min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                 sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "# Build the vocabulary\n",
    "model.build_vocab(sentences=x)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "tot_examples = model.corpus_count\n",
    "# Train the model\n",
    "model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "            queue_factor=2, report_delay=0.0)\n",
    "if not demonstrative:\n",
    "    model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-morgan",
   "metadata": {},
   "source": [
    "# <b>DarkVec 5 days</b>  <a name=\"darkvec5\"></a>\n",
    "[Back to Index](#toc_)\n",
    "\n",
    "Model training for the state of art comparison. Darkvec here is trained on the last 5 days with the per-service language definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    C = 25\n",
    "    V = 50\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 5\n",
    "LANGUAGE = 'xserv'\n",
    "\n",
    "model_name = f'fivedays{C}_v{V}_iter{EPOCHS}'\n",
    "model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "# Define the path for streaming training\n",
    "x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "# Init a new model\n",
    "model = Word2Vec(min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                 sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "# Build the vocabulary\n",
    "model.build_vocab(sentences=x)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "tot_examples = model.corpus_count\n",
    "# Train the model\n",
    "model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "            queue_factor=2, report_delay=0.0)\n",
    "if not demonstrative:\n",
    "    model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-toolbox",
   "metadata": {},
   "source": [
    "# <b>DANTE Training</b> <a name=\"dante\"></a>\n",
    "[Back to Index](#toc_)\n",
    "\n",
    "Word2Vec model training. The model is implemented following the DANTE original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    C = 25\n",
    "    V = 50\n",
    "    EPOCHS = 10\n",
    "DAYS = 5\n",
    "METHODOLOGY = 'dante'\n",
    "\n",
    "model_name = f'{METHODOLOGY}_c{C}_v{V}_iter{EPOCHS}'\n",
    "model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "# Define the path for streaming training\n",
    "x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}', max_sentence_length=100000000);\n",
    "# Init a new model\n",
    "model = Word2Vec(min_count = 0, workers = multiprocessing.cpu_count(),\n",
    "                 sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "# Build the vocabulary\n",
    "model.build_vocab(sentences=x)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "tot_examples = model.corpus_count\n",
    "# Train the model\n",
    "model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "            queue_factor=2, report_delay=0.0)\n",
    "if not demonstrative:\n",
    "    model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "employed-discretion",
   "metadata": {},
   "source": [
    "Extraction of DANTE logs during corpus loading:\n",
    "\n",
    "[...]\n",
    "INFO : PROGRESS: at sentence #104260000, processed 7199935012 words, keeping 42135 word types\n",
    "INFO : PROGRESS: at sentence #104270000, processed 7200855910 words, keeping 42135 word types\n",
    "INFO : PROGRESS: at sentence #104280000, processed 7201857738 words, keeping 42135 word types\n",
    "[...]\n",
    "\n",
    "According to the huge number of samples, we are not able to finish nor the training, nor the data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-liberal",
   "metadata": {},
   "source": [
    "# <b>IP2VEC Training</b> <a name=\"ip2vec\"></a>  \n",
    "\n",
    "[Back to Index](#toc_)\n",
    "\n",
    "Keras-based model training. The model is implemented following the IP2VEC original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "skilled-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(data, w2v):\n",
    "    \"\"\"Extract the IP2VEC corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        dataset\n",
    "    w2v : dict\n",
    "        word to embedding lookup\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        tokens constituting the corpus\n",
    "    \"\"\"\n",
    "    corpus = [[w2v[w] for w in ww]  for ww in data]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ranking-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "if demonstrative:\n",
    "    files = glob(f'{CORPUS}/ip2vec5/*.npz')[:2]\n",
    "else:\n",
    "    files = glob(f'{CORPUS}/ip2vec5/*.npz')\n",
    "# Get target words\n",
    "x = np.concatenate([np.load(a)['x'] for a in files])\n",
    "# Get context words\n",
    "y = np.concatenate([np.load(a)['y'] for a in files])\n",
    "merged = set(x).union(set(y))\n",
    "# Tokenize distinct IPs\n",
    "v2w = {v:cnt for v,cnt in enumerate(sorted(merged))}\n",
    "w2v = {v:k for k,v in v2w.items()}\n",
    "# Merge target words and context words\n",
    "data = [[x, y] for x, y in zip(x, y)]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "silver-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the full corpus\n",
    "corpus = pd.DataFrame(extract_corpus(data, w2v)).to_numpy()\n",
    "# 10% of negative sampling\n",
    "ns = int(10*corpus.shape[0]/100)\n",
    "clist = corpus.tolist()\n",
    "clist = {f'{a[0]}_{a[1]}' for a in clist}\n",
    "negative_x = []\n",
    "negative_y = []\n",
    "negative = set()\n",
    "cnt = 0\n",
    "while cnt < ns:\n",
    "    idx1 = np.random.randint(0, corpus.shape[0])\n",
    "    idx2 = np.random.randint(0, corpus.shape[0])\n",
    "    if f'{corpus[idx1, 0]}_{corpus[idx2, 1]}' in clist and f'{corpus[idx1, 0]}_{corpus[idx2, 1]}' not in negative:\n",
    "        negative.add(f'{corpus[idx1, 0]}_{corpus[idx2, 1]}')\n",
    "        negative_x.append(corpus[idx1, 0])\n",
    "        negative_y.append(corpus[idx2, 1])\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "seasonal-campbell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27252</td>\n",
       "      <td>5637</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20859</td>\n",
       "      <td>26142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8030</td>\n",
       "      <td>5658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27582</td>\n",
       "      <td>13213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27713</td>\n",
       "      <td>30338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  context  label\n",
       "0   27252     5637      1\n",
       "1   20859    26142      1\n",
       "2    8030     5658      1\n",
       "3   27582    13213      1\n",
       "4   27713    30338      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final IP2VEC training dataset\n",
    "neg = pd.DataFrame([zipped for zipped in zip(negative_x, negative_y)], columns=['target', 'context'])\n",
    "neg['label'] = 0\n",
    "pos = pd.DataFrame(corpus, columns=['target', 'context'])\n",
    "pos['label'] = 1\n",
    "data = shuffle(neg.append(pos)).reset_index().drop(columns=['index'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "agreed-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 32)        970880      input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 1)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 1)        0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           reshape_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 970,882\n",
      "Trainable params: 970,882\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1, 32)             970880    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 32, 1)             0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 32)                0         \n",
      "=================================================================\n",
      "Total params: 970,880\n",
      "Trainable params: 970,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define IP2VEC architecture\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(len(w2v), 32, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((32, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((32, 1))(context)\n",
    "\n",
    "dot_product = Dot(axes=1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model([input_target, input_context], output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "model.summary()\n",
    "\n",
    "embedder = Model(input_target, Reshape((32,))(target))\n",
    "embedder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "x1 = data.target.values\n",
    "x2 = data.context.values\n",
    "y = data.label\n",
    "\n",
    "if demonstrative:\n",
    "    model.fit([x1, x2], y, batch_size=1024, epochs=1)\n",
    "else:\n",
    "    model.fit([x1, x2], y, batch_size=1024, epochs=10)\n",
    "\n",
    "if not demonstrative:\n",
    "    print('Model Saved')\n",
    "    embedder.save('{MODELS}/ip2vec5embedder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
