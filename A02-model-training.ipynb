{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "operational-capacity",
   "metadata": {},
   "source": [
    "# <b>DarkVec: Automatic Analysis of Darknet Trafficwith Word Embeddings</b>\n",
    "## <b>Appendix 2: Models Training</b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-minnesota",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# <b>Table of Content</b> <a id=\"toc_\"></a>\n",
    "\n",
    "* [<b>DarkVec Training</b>](#darkvec)  \n",
    "    * [Per-Service Language](#xserv)  \n",
    "    * [Auto-Defined Language](#auto)  \n",
    "    * [Single Language](#single)  \n",
    "* [<b>DarkVec 5 Days</b>](#darkvec5)\n",
    "* [<b>DANTE training</b>](#dante)  \n",
    "* [<b>IP2VEC training</b>](#ip2vec)  \n",
    "\n",
    "In this notebook we report the snippets for training and saving the models used in the paper. The corpus is loaded from the `CORPUS` path in the configuration file. Models and scalers are saved in the `MODELS` path in the configuration file.\n",
    "\n",
    "The model names are related to the parameters _C_, or context window size, and _V_ or embeddings size. Namely, they are:\n",
    "* `single_cC_vV_iter20`: DarkVec model trained on 30 days. Single language;\n",
    "* `auto_cC_vV_iter20`: DarkVec model trained on 30 days. Auto-defined languages;\n",
    "* `service_cC_vV_iter20`: DarkVec model trained on 30 days. Per-service languages;\n",
    "* `fivedays_c25_v50_iter20`: DarkVec model trained on 5 days. Per-service languages;\n",
    "* `ip2vec5embedder`: keras embedder generated through our implementation following the IP2VEC paper.\n",
    "\n",
    "\n",
    "___\n",
    "***Note:*** All the code and data we provide are the ones included in the paper. To speed up the notebook execution, by default we trim the files when reading them. Comments on how to run on complete files are provided in the notebook. Note that running the notebook with the complete dataset requires *a PC with significant amount of memory*. \n",
    "\n",
    "***Note:*** Be aware that these script require a large amount of time for training all models tested in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import multiprocessing\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Reshape, Dot\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import logging\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-nicholas",
   "metadata": {},
   "source": [
    "# <b>Darkvec Training</b> <a name=\"darkvec\"></a>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrative = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-marathon",
   "metadata": {},
   "source": [
    "### <b>Per-Service Language</b>  <a name=\"xserv\"></a>\n",
    "\n",
    "\n",
    "\n",
    "Grid search DarkVec training over 30 days of traffic. Per-service language means that each sequence of IPs is extracted with respect to the classes of service (port/protocol) pairs. The language definition is stored in the `SERVICES` path of the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    Cs = [5, 25, 50, 75]\n",
    "    Vs = [50, 100, 150, 200]\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 30\n",
    "LANGUAGE = 'xserv'\n",
    "\n",
    "for C in Cs:\n",
    "    for V in Vs:\n",
    "        model_name = f'service_c{C}_v{V}_iter{EPOCHS}'\n",
    "        model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "        # Define the path for streaming training\n",
    "        x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "        # Init a new model\n",
    "        model = Word2Vec(sentences=None, min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                         sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "        # Build the vocabulary\n",
    "        model.build_vocab(sentences=x)\n",
    "        vocab_size = len(model.wv.vocab)\n",
    "        tot_examples = model.corpus_count\n",
    "        # Train the model\n",
    "        model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "                    queue_factor=2, report_delay=0.0)\n",
    "        if not demonstrative:\n",
    "            model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-composite",
   "metadata": {},
   "source": [
    "### <b>Auto-Defined Language</b> <a id=\"auto\"></a>\n",
    "\n",
    "\n",
    "Grid search DarkVec training over 30 days of traffic. Aut-defined language means that each sequence of IPs is extracted with respect to top-10 (port/protocol) pairs. All the other pairs are classified as _oth_. The language definition is stored in the `SERVICES` path of the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    Cs = [5, 25, 50, 75]\n",
    "    Vs = [50, 100, 150, 200]\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 30\n",
    "LANGUAGE = 'auto'\n",
    "\n",
    "for C in Cs:\n",
    "    for V in Vs:\n",
    "        model_name = f'auto_c{C}_v{V}_iter{EPOCHS}'\n",
    "        model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "        # Define the path for streaming training\n",
    "        x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "        # Init a new model\n",
    "        model = Word2Vec(min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                         sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "        # Build the vocabulary\n",
    "        model.build_vocab(sentences=x)\n",
    "        vocab_size = len(model.wv.vocab)\n",
    "        tot_examples = model.corpus_count\n",
    "        # Train the model\n",
    "        model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "                    queue_factor=2, report_delay=0.0)\n",
    "        if not demonstrative:\n",
    "            model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-ordinary",
   "metadata": {},
   "source": [
    "### <b>Single Language</b> <a name=\"single\"></a>\n",
    "\n",
    "\n",
    "Grid search DarkVec training over 30 days of traffic. Single language means that each sequence of IPs is extracted exactly as they reached the darknet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    C = 75\n",
    "    V = 50\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 30\n",
    "LANGUAGE = 'single'\n",
    "\n",
    "model_name = f'single_c{C}_v{V}_iter{EPOCHS}'\n",
    "model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "# Define the path for streaming training\n",
    "x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "# Init a new model\n",
    "model = Word2Vec(min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                 sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "# Build the vocabulary\n",
    "model.build_vocab(sentences=x)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "tot_examples = model.corpus_count\n",
    "# Train the model\n",
    "model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "            queue_factor=2, report_delay=0.0)\n",
    "if not demonstrative:\n",
    "    model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-crack",
   "metadata": {},
   "source": [
    "# <b>DarkVec 5 days</b>  <a name=\"darkvec5\"></a>\n",
    "\n",
    "\n",
    "Model training for the state of art comparison. Darkvec here is trained on the last 5 days with the per-service language definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    C = 25\n",
    "    V = 50\n",
    "    EPOCHS = 20\n",
    "METHODOLOGY = 'darkvec'\n",
    "DAYS = 5\n",
    "LANGUAGE = 'xserv'\n",
    "\n",
    "model_name = f'fivedays{C}_v{V}_iter{EPOCHS}'\n",
    "model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "# Define the path for streaming training\n",
    "x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}{LANGUAGE}', max_sentence_length=100000000);\n",
    "# Init a new model\n",
    "model = Word2Vec(min_count = 10, workers = multiprocessing.cpu_count(),\n",
    "                 sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "# Build the vocabulary\n",
    "model.build_vocab(sentences=x)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "tot_examples = model.corpus_count\n",
    "# Train the model\n",
    "model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "            queue_factor=2, report_delay=0.0)\n",
    "if not demonstrative:\n",
    "    model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-elephant",
   "metadata": {},
   "source": [
    "# <b>DANTE Training</b> <a name=\"dante\"></a>\n",
    "\n",
    "\n",
    "Word2Vec model training. The model is implemented following the DANTE original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify models parameter\n",
    "if demonstrative:\n",
    "    Cs = [5]\n",
    "    Vs = [10]\n",
    "    EPOCHS = 1\n",
    "else:\n",
    "    C = 25\n",
    "    V = 50\n",
    "    EPOCHS = 10\n",
    "DAYS = 5\n",
    "METHODOLOGY = 'dante'\n",
    "\n",
    "model_name = f'{METHODOLOGY}_c{C}_v{V}_iter{EPOCHS}'\n",
    "model_path_name = f'{MODELS}/{model_name}.model'\n",
    "\n",
    "# Define the path for streaming training\n",
    "x = PathLineSentences(f'{CORPUS}/{METHODOLOGY}{DAYS}', max_sentence_length=100000000);\n",
    "# Init a new model\n",
    "model = Word2Vec(min_count = 0, workers = multiprocessing.cpu_count(),\n",
    "                 sg = 1, size = V, window = C, sample = 0, seed = 15)\n",
    "# Build the vocabulary\n",
    "model.build_vocab(sentences=x)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "tot_examples = model.corpus_count\n",
    "# Train the model\n",
    "model.train(x, total_examples=tot_examples, epochs=EPOCHS, \n",
    "            queue_factor=2, report_delay=0.0)\n",
    "if not demonstrative:\n",
    "    model.save(model_path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-magnitude",
   "metadata": {},
   "source": [
    "    Extraction of DANTE logs during corpus loading:\n",
    "\n",
    "    [...]\n",
    "    INFO : PROGRESS: at sentence #104260000, processed 7199935012 words, keeping 42135 word types\n",
    "    INFO : PROGRESS: at sentence #104270000, processed 7200855910 words, keeping 42135 word types\n",
    "    INFO : PROGRESS: at sentence #104280000, processed 7201857738 words, keeping 42135 word types\n",
    "    [...]\n",
    "\n",
    "    According to the huge number of samples, we are not able to finish nor the training, nor the data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-hospital",
   "metadata": {},
   "source": [
    "# <b>IP2VEC Training</b> <a name=\"ip2vec\"></a>  \n",
    "\n",
    "\n",
    "\n",
    "Keras-based model training. The model is implemented following the IP2VEC original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus(data, w2v):\n",
    "    \"\"\"Extract the IP2VEC corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        dataset\n",
    "    w2v : dict\n",
    "        word to embedding lookup\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        tokens constituting the corpus\n",
    "    \"\"\"\n",
    "    corpus = [[w2v[w] for w in ww]  for ww in data]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "if demonstrative:\n",
    "    files = glob(f'{CORPUS}/ip2vec5/*.npz')[:2]\n",
    "else:\n",
    "    files = glob(f'{CORPUS}/ip2vec5/*.npz')\n",
    "# Get target words\n",
    "x = np.concatenate([np.load(a)['x'] for a in files])\n",
    "# Get context words\n",
    "y = np.concatenate([np.load(a)['y'] for a in files])\n",
    "merged = set(x).union(set(y))\n",
    "# Tokenize distinct IPs\n",
    "v2w = {v:cnt for v,cnt in enumerate(sorted(merged))}\n",
    "w2v = {v:k for k,v in v2w.items()}\n",
    "# Merge target words and context words\n",
    "data = [[x, y] for x, y in zip(x, y)]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the full corpus\n",
    "corpus = pd.DataFrame(extract_corpus(data, w2v)).to_numpy()\n",
    "# 10% of negative sampling\n",
    "ns = int(10*corpus.shape[0]/100)\n",
    "clist = corpus.tolist()\n",
    "clist = {f'{a[0]}_{a[1]}' for a in clist}\n",
    "negative_x = []\n",
    "negative_y = []\n",
    "negative = set()\n",
    "cnt = 0\n",
    "while cnt < ns:\n",
    "    idx1 = np.random.randint(0, corpus.shape[0])\n",
    "    idx2 = np.random.randint(0, corpus.shape[0])\n",
    "    if f'{corpus[idx1, 0]}_{corpus[idx2, 1]}' in clist and f'{corpus[idx1, 0]}_{corpus[idx2, 1]}' not in negative:\n",
    "        negative.add(f'{corpus[idx1, 0]}_{corpus[idx2, 1]}')\n",
    "        negative_x.append(corpus[idx1, 0])\n",
    "        negative_y.append(corpus[idx2, 1])\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final IP2VEC training dataset\n",
    "neg = pd.DataFrame([zipped for zipped in zip(negative_x, negative_y)], columns=['target', 'context'])\n",
    "neg['label'] = 0\n",
    "pos = pd.DataFrame(corpus, columns=['target', 'context'])\n",
    "pos['label'] = 1\n",
    "data = shuffle(neg.append(pos)).reset_index().drop(columns=['index'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IP2VEC architecture\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(len(w2v), 32, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((32, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((32, 1))(context)\n",
    "\n",
    "dot_product = Dot(axes=1)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "\n",
    "model = Model([input_target, input_context], output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "model.summary()\n",
    "\n",
    "embedder = Model(input_target, Reshape((32,))(target))\n",
    "embedder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "x1 = data.target.values\n",
    "x2 = data.context.values\n",
    "y = data.label\n",
    "\n",
    "if demonstrative:\n",
    "    model.fit([x1, x2], y, batch_size=1024, epochs=1)\n",
    "else:\n",
    "    model.fit([x1, x2], y, batch_size=1024, epochs=10)\n",
    "\n",
    "if not demonstrative:\n",
    "    print('Model Saved')\n",
    "    embedder.save('{MODELS}/ip2vec5embedder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a1528-ecb4-4599-b5ff-b28fe229df4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
